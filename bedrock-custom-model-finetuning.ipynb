{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28541779-62cc-423b-99f5-92061e00fcbd",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Deploying Custom Models on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a827fab-f3ed-41d2-aa05-363a86c6b72b",
   "metadata": {},
   "source": [
    "### Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400f920-4ea0-4f76-8b69-011965a81d24",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of fine-tuning a Cohere `command-light-text-v14` model on Amazon Bedrock for generating concise clinical notes from patient-doctor conversations. The key steps include resource setup, data preparation, fine-tuning, and deployment. For a detailed explanation of the fine-tuning process, including the use case and configurations, please refer to the accompanying article: [Fine-Tuning and Deploying Custom AI Models on Amazon Bedrock: A Practical Guide](https://dev.to/miladrezaei/fine-tuning-and-deploying-custom-ai-models-on-amazon-bedrock-a-practical-guide-39m6).\n",
    "\n",
    "The dataset used in this notebook is a **combined version of the ACI-Bench training and validation datasets**:\n",
    "- **Source**: [ACI-Bench: Ambient Clinical Intelligence Benchmark](http://github.com/microsoft/clinical_visit_note_summarization_corpus).\n",
    "- **Contents**: Patient-doctor dialogues paired with concise clinical notes summarizing the interaction.\n",
    "\n",
    "The combined dataset contains **88 examples**, which is intentionally small for this demonstration.\n",
    "\n",
    "### Why a Smaller Dataset?\n",
    "For the purpose of this guideline, I have chosen a smaller dataset to:\n",
    "- **Reduce the cost of fine-tuning**: Fine-tuning on large datasets can be computationally expensive. Using a small dataset minimizes resource consumption, making it practical for a demonstration.\n",
    "- **Simplify the workflow**: A smaller dataset allows us to focus on the technical steps involved in fine-tuning without the overhead of managing large-scale data.\n",
    "\n",
    "However, in a real-world scenario, **fine-tuning tasks typically require thousands or tens of thousands of examples** to achieve meaningful results. Future steps may include:\n",
    "- Expanding the dataset with additional clinical dialogues.\n",
    "- Augmenting the data using paraphrasing, back-translation, or similar techniques.\n",
    "\n",
    "### Use Case\n",
    "The fine-tuned model will be trained to:\n",
    "- Understand the context of medical dialogues.\n",
    "- Generate concise, structured clinical notes summarizing key patient symptoms and doctor recommendations.\n",
    "\n",
    "This use case is particularly relevant for:\n",
    "- **Healthcare providers**: Automating medical documentation and reducing administrative overhead.\n",
    "- **Telemedicine**: Summarizing virtual consultations for patient records.\n",
    "- **Clinical NLP**: Building AI systems for clinical note generation.\n",
    "\n",
    "### Dataset Preparation\n",
    "1. **Combination**:\n",
    "   - The original ACI-Bench training (`train.csv`) and validation (`valid.csv`) datasets were combined to create a single dataset of 88 examples.\n",
    "2. **Preprocessing**:\n",
    "   - The dialogues were used as **prompts**, and their corresponding notes were used as **completions** to create input-output pairs.\n",
    "3. **Format**:\n",
    "   - The processed dataset is saved in JSONL format to prepare it for fine-tuning on Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c37bd4-e115-4bd1-8667-60cc3705f549",
   "metadata": {},
   "source": [
    "## Technical Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d96bd-49c2-4641-84e0-73c23d2750c8",
   "metadata": {},
   "source": [
    "- Access to Amazon Bedrock with Cohere command-light-text-v14 model enabled\n",
    "- AWS SDK (boto3) configured with appropriate IAM permissions\n",
    "- Python environment with required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e6f03-5226-4696-a2ec-aeb54cd06a24",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36faee07-2d4f-4bad-b9a8-1f0efac96b25",
   "metadata": {},
   "source": [
    "We begin by installing and importing the required libraries for handling data processing, AWS interactions, and API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a3e4-1aac-491a-b5c5-417baae46e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets boto3 pandas json\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca73c7d-edf3-40b8-a57e-69f73b137d1c",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d06e4-7030-4562-9aaf-44d5147e60f0",
   "metadata": {},
   "source": [
    "In this step, we load the combined dataset from a CSV file to inspect its structure and size before formatting it for fine-tuning.\n",
    "\n",
    "- **Dataset Source**: The dataset is a consolidated version of training and validation data from ACI-Bench.\n",
    "- **Objective**: The dataset contains doctor-patient dialogues (`dialogue`) and corresponding clinical notes (`note`). These will be reformatted into prompt-completion pairs for fine-tuning.\n",
    "- **Validation**: By checking the dataset size and column names, we ensure that the data is correctly loaded and aligned with the required format for downstream tasks.\n",
    "\n",
    "This validation step is critical to avoid errors during processing and to confirm that the dataset matches the expected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b682c2-85b7-47ae-b1b5-286ff1686c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset splits and path\n",
    "train_dataset_path = \"dataset/data.csv\"\n",
    "\n",
    "train_dataset = pd.read_csv(train_dataset_path)\n",
    "\n",
    "# Check the dataset size and structure\n",
    "print(f\"Total records in the dataset: {len(train_dataset)}\")\n",
    "print(f\"Columns in the dataset: {train_dataset.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3a51e-f93d-43fd-ad5f-494b156501df",
   "metadata": {},
   "source": [
    "Once the dataset is loaded and validated, we format it into the JSONL structure required for fine-tuning. This involves the following steps:\n",
    "\n",
    "1. **Defining the Output Path**:\n",
    "   - The reformatted dataset will be saved as a JSONL file in the `dataset` directory.\n",
    "   - JSONL (JSON Lines) format is preferred for fine-tuning as it allows efficient storage of individual prompt-completion pairs.\n",
    "\n",
    "2. **Formatting Data**:\n",
    "   - Each row in the dataset is converted into a dictionary with:\n",
    "     - `prompt`: Contains the patient-doctor dialogue prefixed with the instruction: \"Summarize the following conversation.\"\n",
    "     - `completion`: Contains the corresponding clinical note summarizing the dialogue.\n",
    "   - This structure aligns with the format expected by fine-tuning frameworks like Amazon Bedrock.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c70f27-01d5-4f6e-ba4e-91c31ceaedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path for JSONL\n",
    "output_file_name = 'clinical_notes_fine_tune.jsonl'\n",
    "output_file_path = os.path.join('dataset', output_file_name)\n",
    "output_dir = os.path.dirname(output_file_path)\n",
    "\n",
    "# Prepare and save the dataset in the fine-tuning JSONL format\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for _, row in train_dataset.iterrows():\n",
    "        formatted_entry = {\n",
    "            \"completion\": row['note'],  # Replace 'note' with the correct column name\n",
    "            \"prompt\": f\"Summarize the following conversation.\\n\\n{row['dialogue']}\"  # Replace 'dialogue' as needed\n",
    "        }\n",
    "        json.dump(formatted_entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "    print(f\"Dataset has been reformatted and saved to {output_file_path}.\")\n",
    "\n",
    "# Optional: Print example formatted entry for debugging\n",
    "example_formatted_entry = {\n",
    "    \"completion\": train_dataset.iloc[0]['note'],  # Replace with the correct column name\n",
    "    \"prompt\": f\"Summarize the following conversation.\\n\\n{train_dataset.iloc[0]['dialogue']}\"  # Replace as needed\n",
    "}\n",
    "print(\"Example formatted entry:\")\n",
    "print(json.dumps(example_formatted_entry, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dc416-aca7-491c-94f2-02a47b4db306",
   "metadata": {},
   "source": [
    "### Step 3: Upload the Dataset to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2f1d9-8fb6-4250-b7ae-4b7843bb1a55",
   "metadata": {},
   "source": [
    "This step uploads the prepared JSONL dataset to an S3 bucket, making it accessible for fine-tuning with Amazon Bedrock.\n",
    "\n",
    "1. **Bucket Setup**:\n",
    "   - Checks if the specified S3 bucket (`bucket_name`) exists. If not, it creates the bucket in the specified AWS region (`region`).\n",
    "\n",
    "2. **File Upload**:\n",
    "   - The dataset is uploaded to the S3 bucket at the specified key (`s3_key`).\n",
    "   - The final S3 location will be `s3://<bucket_name>/<s3_key>`.\n",
    "\n",
    "### Notes:\n",
    "- Ensure your AWS region and credentials are configured correctly.\n",
    "- Update `bucket_name` and `region` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589468f-b27f-4fd5-b3ff-bcb1998b1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and S3 details\n",
    "bucket_name = 'bedrock-finetuning-bucket25112024' # This bucket has to be created \n",
    "s3_key = output_file_name\n",
    "\n",
    "# Specify the region\n",
    "region = 'us-east-1'  # Change this if needed\n",
    "\n",
    "# Initialize S3 client with the specified region\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Check if the bucket exists\n",
    "try:\n",
    "    existing_buckets = s3_client.list_buckets()\n",
    "    bucket_exists = any(bucket['Name'] == bucket_name for bucket in existing_buckets['Buckets'])\n",
    "\n",
    "    if not bucket_exists:\n",
    "        # Create the bucket based on the region\n",
    "        try:\n",
    "            if bucket_region == 'us-east-1':\n",
    "                # For us-east-1, do not specify LocationConstraint\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "                print(f\"Bucket {bucket_name} created successfully in us-east-1.\")\n",
    "            else:\n",
    "                # For other regions, specify the LocationConstraint\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': bucket_region}\n",
    "                )\n",
    "                print(f\"Bucket {bucket_name} created successfully in {bucket_region}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket: {e}\")\n",
    "            raise e\n",
    "    else:\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "\n",
    "    # Upload the file to S3\n",
    "    try:\n",
    "        s3_client.upload_file(output_file_path, bucket_name, s3_key)\n",
    "        print(f\"File uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to S3: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56295356-ebc3-4029-87b3-052e1894f58c",
   "metadata": {},
   "source": [
    "## Step 5: Fine-Tune the Model on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601212c-26a1-4307-a667-4cb7789d33e2",
   "metadata": {},
   "source": [
    "This step initializes the Amazon Bedrock client and retrieves a list of foundation models available for fine-tuning. The Bedrock client is set up for the specified AWS region, ensuring compatibility with the service. Using the `list_foundation_models` method, the script identifies models that support fine-tuning and displays their details, such as `modelId` and `modelName`. This information helps in selecting the appropriate model for the fine-tuning process.\n",
    "\n",
    "Ensure the AWS region is correctly configured and that you have the necessary permissions to access Bedrock services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c860c0-0790-42bf-85da-3d4f82e110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the AWS region and initialize the Bedrock client\n",
    "bedrock_region = \"us-east-1\"\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=bedrock_region)\n",
    "\n",
    "# List foundation models available for fine-tuning\n",
    "response = bedrock.list_foundation_models(byCustomizationType=\"FINE_TUNING\")\n",
    "\n",
    "# Display the available models\n",
    "for model in response[\"modelSummaries\"]:\n",
    "    print(f\"Model ID: {model['modelId']}\")\n",
    "    print(f\"Model Name: {model['modelName']}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475599f-c084-438f-b405-b7f447fd3ed5",
   "metadata": {},
   "source": [
    "## Step 6: Create an IAM Role for Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f54708-d343-46f1-9816-1812f04d60e2",
   "metadata": {},
   "source": [
    "This step sets up an IAM role with the required permissions to facilitate the fine-tuning process on Amazon Bedrock. The role is configured with a trust policy, allowing the Bedrock service to assume the role, and permissions to access the S3 bucket where the fine-tuning dataset is stored.\n",
    "\n",
    "- **Role Definition**:\n",
    "  - The role is created with a trust policy specifying `bedrock.amazonaws.com` as the trusted service. This allows Bedrock to use the role for the fine-tuning process.\n",
    "\n",
    "- **Permissions**:\n",
    "  - A custom policy is attached to the role, granting it access to perform `s3:GetObject`, `s3:PutObject`, and `s3:ListBucket` actions on the specified S3 bucket.\n",
    "\n",
    "- **Result**:\n",
    "  - The role's ARN (`role_arn`) is displayed upon successful creation, which can then be used to configure the fine-tuning job.\n",
    "\n",
    "### Notes:\n",
    "- Replace `bucket_name` with the name of your S3 bucket containing the dataset.\n",
    "- Ensure your AWS credentials have sufficient permissions to create IAM roles and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150afdd-8f23-4066-9ed7-a4905b03d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IAM client\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Define role name and trust policy\n",
    "role_name = \"BedrockFineTuningRole\" # Customizable: Name of the IAM role\n",
    "role_description = \"Role for Bedrock fine-tuning job\" # Customizable: Description of the IAM role\n",
    "\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=role_description\n",
    "    )\n",
    "    role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role with ARN: {role_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating role: {e}\")\n",
    "\n",
    "# Attach permission policies to allow access to S3\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{bucket_name}\",\n",
    "                f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "iam_client.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=\"BedrockFineTuningS3Policy\",\n",
    "    PolicyDocument=json.dumps(permission_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33fd5a-bc0b-4786-a8ed-d5728395b3b8",
   "metadata": {},
   "source": [
    "## Step 7: Submit the Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50ecd1-bc37-46c5-831a-7b98e683ffe2",
   "metadata": {},
   "source": [
    "- **Job Parameters**:\n",
    "  - `base_model_id`: The identifier for the foundation model being fine-tuned. Here, it's set to `cohere.command-light-text-v14:7:4k`, which is suitable for summarization tasks.\n",
    "  - `job_name`: A unique name for the fine-tuning job, useful for tracking.\n",
    "  - `model_name`: The name of the custom fine-tuned model that will be created.\n",
    "\n",
    "- **IAM Role**:\n",
    "  - `roleArn`: The ARN of the IAM role created earlier, allowing Bedrock to access S3 resources for training.\n",
    "\n",
    "- **Hyperparameters**:\n",
    "  - `epochCount`: Number of epochs for training. Adjust based on convergence and overfitting.\n",
    "  - `batchSize`: Number of samples per batch. Tune this based on memory and training speed.\n",
    "  - `learningRate`: Learning rate for training. Lower values can improve stability.\n",
    "\n",
    "- **Data Configuration**:\n",
    "  - `trainingDataConfig`: Specifies the S3 URI where the training dataset is stored.\n",
    "  - `outputDataConfig`: Specifies the S3 URI where the fine-tuned model will be saved.\n",
    "\n",
    "### Notes:\n",
    "- Ensure the S3 bucket is accessible with the appropriate permissions.\n",
    "- Monitor the job in the AWS console under Amazon Bedrock to track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7adc3-c0fc-4d71-8761-7da8074a90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the job parameters\n",
    "base_model_id = \"cohere.command-light-text-v14:7:4k\"\n",
    "job_name = \"cohere-medical-summary-finetune-job-v1\" # Customizable: Name of the Finetuning job\n",
    "model_name = \"cohere-Summarizer-medical-Tuned-v1\"\n",
    "\n",
    "# Submit the fine-tuning job\n",
    "bedrock.create_model_customization_job(\n",
    "    customizationType=\"FINE_TUNING\",\n",
    "    jobName=job_name,\n",
    "    customModelName=model_name,\n",
    "    roleArn=role_arn,\n",
    "    baseModelIdentifier=base_model_id,\n",
    "    hyperParameters={\n",
    "        \"epochCount\": \"3\", # Adjust based on convergence and overfitting\n",
    "        \"batchSize\": \"16\", # Adjust based on memory availability and training speed\n",
    "        \"learningRate\": \"0.00005\", # Adjust based on training stability and speed\n",
    "    },\n",
    "    trainingDataConfig={\"s3Uri\": f\"s3://{bucket_name}/{s3_key}\"},\n",
    "    outputDataConfig={\"s3Uri\": f\"s3://{bucket_name}/finetuned/\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62221d-3367-49a2-b71b-7262625936e6",
   "metadata": {},
   "source": [
    "You can check the job status (next step) to make sure if it is finished or still being trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca9faa-b5fb-4139-ae32-aff148f4a091",
   "metadata": {},
   "source": [
    "## Step 8: Monitor the Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fae14d-6587-442c-98f6-74d313055833",
   "metadata": {},
   "source": [
    "This step checks the status of the fine-tuning job submitted to Amazon Bedrock. Monitoring the job status is crucial for understanding its progress and determining when it has completed or if any issues have occurred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fc59a-373c-4c9d-b172-a08578d73202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the job status\n",
    "status = bedrock.get_model_customization_job(jobIdentifier=job_name)[\"status\"]\n",
    "print(f\"Job status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c67d3-2464-42bd-b2c9-acfa1b28bd53",
   "metadata": {},
   "source": [
    "## Step 9: Perform Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65278350-aa22-4cde-967b-1ff26ae43eb4",
   "metadata": {},
   "source": [
    "To use the model for inference, you need to purchase \"Provisioned Throughput.\" On Amazon Bedrock sidebar in your AWS console, go to \"Custom Models\" and then choose the \"Models\" tab, select the model you have trained, and then click on \"Purchase Provisioned Throughput.\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e2c1b33-35fd-4a37-a9b8-cdf89fd01e9c",
   "metadata": {},
   "source": [
    "![Provisioned Throughput](images/provisioned-throughput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355aed7-7435-47a6-95cf-cbedd21e0285",
   "metadata": {},
   "source": [
    "Give the provisioned throughput a name, select a commitment term (you can choose \"No Commitment\" for testing), and then click \"Purchase Provisioned Throughput.\" You will be able to see the estimated price as well. Once this is set up, you'll be able to use the model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c083f-8b35-412e-8344-6db0527781ee",
   "metadata": {},
   "source": [
    "![Commitment term](images/commitment-term.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e4dbf-f45e-4c6a-96c0-7ea2210ada70",
   "metadata": {},
   "source": [
    "Give the provisioned throughput a name, select a commitment term (you can choose \"No Commitment\" for testing), and then click \"Purchase Provisioned Throughput.\" Once this is set up, you'll be able to use the model for inference. You can also see the estimated price for each commitment term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13c4ee-ff13-4513-8bde-05704f1f0e9d",
   "metadata": {},
   "source": [
    "![Alt text](images/provision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c045b-a209-4471-8d78-b8c29f50f5c7",
   "metadata": {},
   "source": [
    "To access your deployed model's endpoint, you'll need its ARN. Go to the \"Provisioned Throughput\" section under Inference in the sidebar. Select the name of your fine-tuned model, and on the new page, copy the ARN for use in the next step. Keep in mind that provisioning throughput may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab7949-d919-4b2f-930c-e74fd6f249d4",
   "metadata": {},
   "source": [
    "![Custom Model ARN](images/custom-model-arn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf4c35-7eb8-48e6-804f-e79dd9af70e0",
   "metadata": {},
   "source": [
    "In the next step, we will make a request to the model for inference. Be sure to replace YOUR_MODEL_ARN with the ARN you copied in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c6e79-ab30-4671-bb4a-dc947d0ce3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock runtime client\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=bedrock_region)\n",
    "\n",
    "# Define a prompt for model inference\n",
    "prompt = \"\"\"\n",
    "[doctor] Good morning, Mr. Smith. How have you been feeling since your last visit?  \n",
    "[patient] Good morning, doctor. I've been okay overall, but I’ve been struggling with persistent fatigue and some dizziness.  \n",
    "[doctor] I see. Is the dizziness occurring frequently or only under specific circumstances?  \n",
    "[patient] It’s mostly when I stand up quickly or after I've been walking for a while.  \n",
    "[doctor] Have you noticed any changes in your heart rate or shortness of breath during these episodes?  \n",
    "[patient] No shortness of breath, but I do feel my heart racing sometimes.  \n",
    "\n",
    "[doctor] How about your medications? Are you taking them as prescribed?  \n",
    "[patient] Yes, but I missed a few doses of my beta-blocker last week due to travel.  \n",
    "[doctor] That could explain some of the symptoms. I’ll need to check your blood pressure and do an EKG to assess your heart rhythm.  \n",
    "[patient] Okay, doctor.  \n",
    "\n",
    "[doctor] How has your diet been? Are you still following the low-sodium plan we discussed?  \n",
    "[patient] I’ve been trying, but I’ve slipped up a bit during holidays with family meals.  \n",
    "[doctor] I understand. We’ll reinforce that, as it’s critical for managing your hypertension.  \n",
    "[patient] Yes, I’ll make sure to get back on track.  \n",
    "\n",
    "[doctor] Let’s discuss the results from your last bloodwork. Your cholesterol levels were slightly elevated, and your hemoglobin A1c suggests borderline diabetes.  \n",
    "[patient] I see. What does that mean for me?  \n",
    "[doctor] It means we need to focus on dietary changes and consider starting a low-dose statin. I’ll also refer you to a nutritionist for better meal planning.  \n",
    "[patient] That makes sense. Thank you, doctor.  \n",
    "\n",
    "[doctor] Lastly, you mentioned experiencing more frequent leg swelling recently. Is that still a concern?  \n",
    "[patient] Yes, especially after long days at work.  \n",
    "[doctor] That could be a sign of fluid retention. I’ll adjust your diuretic dose and monitor your progress over the next two weeks.  \n",
    "[patient] Thank you, doctor.  \n",
    "\n",
    "[doctor] All right, let’s get those tests done and review everything at our next appointment. Do you have any other concerns?  \n",
    "[patient] No, I think that’s all for now.  \n",
    "[doctor] Great. See you in two weeks. \n",
    "\"\"\"\n",
    "\n",
    "# Define the inference request body\n",
    "body = {\n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": 0.5,\n",
    "    \"p\": 0.9,\n",
    "    \"max_tokens\": 80,\n",
    "}\n",
    "\n",
    "# Specify the ARN of the custom model\n",
    "custom_model_arn = \"YOUR_MODEL_ARN\" #Put your model ARN here\n",
    "\n",
    "# Invoke the custom model for inference\n",
    "try:\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=custom_model_arn,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "\n",
    "    # Read and parse the response\n",
    "    response_body = response['body'].read().decode('utf-8')\n",
    "    result = json.loads(response_body)\n",
    "\n",
    "    # Extract the summary from the response\n",
    "    summary_text = result['generations'][0]['text']\n",
    "    print(\"Extracted Summary:\", summary_text)\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e5e5e-a4df-4b10-ada6-3535ffee3acd",
   "metadata": {},
   "source": [
    "You can also test the inference directly from the Playground in the Amazon Bedrock console. To do this, navigate to Chat/Text under the Playground section, select your fine-tuned model, and enter your desired prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a7a09-d48a-420d-b155-1b6d2fd9f180",
   "metadata": {},
   "source": [
    "![Playground](images/playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507b7ca-323b-4b40-aa37-268d30b8bbda",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "<span style=\"color:red\">To avoid incurring additional costs, please ensure that you **remove any provisioned throughput**.</span>\n",
    "\n",
    "<span style=\"color:red\">You can remove provisioned throughput by navigating to the **Provisioned Throughput** section from the sidebar in the Amazon Bedrock console. Select the active provisioned throughput and delete it.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345fa64d-379e-4582-813f-e00ae8088d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
